# lake_google_data_analytics_capstone_project
My Capstone Project for the Google Data Analytics program at Coursera

# Google Data Analytics Certificate Capstone Project

## The Adequacy of YouTube Auto-Generated Closed Captions as Data Sources for Formulaic Language Studies

### Task

In the face of a rapidly-changing world economies in the past year, upskilling is crucial. One avenue at which to upskill workers from around the world comes in the form of English for Specific Purposes (ESP) courses. ESP differs from other English classes in their focus solely on English that a learner will encounter in their desired profession.

One issue in designing ESP courses is a source of authentic data that reflects the reality of the profession in which a learner will enter. Fortunately, in the past 30 years, major advancements have been made in corpus linguistics (CL). Broadly speaking, corpus linguistics is an approach which allows teachers and researchers to investigate specific questions about language using large collections of text files to see how patterns repeat. One successful application of this approach is found in the book Write Like a Chemist, which bases all of its lessons on data extracted from a million words of academic chemistry articles. Data from fields besides highly formalized, written texts, however, are needed in order for a wider range of ESP courses to reach learners in need.

Accordingly, the purpose of the present case study is to investigate the extent to which Closed Captions auto-generated by  YouTube can serve as adequate data sources for formulaic research among specialists.

### Data sources used

I used two corpora. 

Brown Corpus:

Wikipedia: "The Brown University Standard Corpus of Present-Day American English (or just Brown Corpus) is an electronic collection of text samples of American English, the first major structured corpus of varied genres."

SGDQ 2019:

Auto-generated closed-caption files for the Summer Games Done Quick 2019 telethon.

THis is a telethon done in partnership with Doctors without Borders, an organization which provides medical cares to areas in need.

TO raise money, experts complete video games at world-record times, often with hundreds of thousands of viewers at a time. They often perform strategies that rely on perfect timing to manipulate the games' memory. Meanwhile, viewers donate. The event consistently raises several millions of dollars for Doctors without Borders.

I have chosen this event as an understudied  genre of texts with whcih I am familiar and that exercises a notable public impact of good.


### Data cleaning & Manipulation

To get the Brown Corpus, I downloaded from [URL]. I used "text separator character" to separate the text into individual text files. 

To get the SGDQ2019 corpus, I obtained a list of URLs for every recording. I then found [program] on GeeksforGeeks. I modified it with the following code to download more than 1 file at a time.

I then opened up all the text files in a software tool which I am developing with my dissertation chair (more details on this program in a few months). The program can output a list of "lexical bundles", or  expressions that recur in a collection of text files.  

### Analysis Summary 

I compared the frequencies of lexical bundles that occur in both the Brown Corpus and the SGDQ2019 Corpus. I ran a histogram inspection and found that the data were not normally distributed. Thus, the appropriate comparison test is a Wilcoxon Sign Ranked test since the data were not normally distributed.

"8_1.txt
 Legend of Zelda The Minish Cap
you don't know this at the time that you normally do this portal but it actually opens a portal to a much later area of the game which *you're not supposed to* have access to at this point this whole what later on in the run let us skip basically directly to the second-to-last engine"


"18_4.txt Fire Emblem Fates

"*that's never happened before* so while I do this chapter which should take like 30 seconds we can read off and not a few donations"
"


Super Mario Bros. 2
4_1.txt

"nice so normally what you do there is climb to *the top of* that screen and bring a key *all the way* down so that *saves a lot of time* "

4_2.txt

Monster Boy in Cursed Kingdom

"so we got a few small puzzles going through here this is pretty straightforward for just a moment mostly *she's gonna* try and hit the torches out of order of what the game is intending but there's probably a moment here"


### Supporting visualizations and key findings

![Boxplot of mean formualicity scores in both corpora](https://github.com/lexicalmichaellake/lake_google_data_analytics_capstone_project/blob/main/all_formualicity_boxplot.png "Boxplot of mean formualicity scores in both corpora").



![Boxplot of mean formualicity scores in LBs shared in corpora](https://github.com/lexicalmichaellake/lake_google_data_analytics_capstone_project/blob/main/shared_formualicity_boxplot.png "Boxplot of mean formualicity scores in LBs shared in corpora").


### Future deliverables 

Cleaning up the transcription of [speedrun] that has the most formulaic sequences and bolding those.


### Your top high-level insights based on your analysis

YouTube auto-generated closed captions are currently inadequate for precise transcriptions, but can serve as a practical, accurate source of data for learning formulaic phrases used amongst specific practitioners.

This knowledge has applications for learning to speak like a:
* cooking show host
* makeup tutorial host
* talk radio host
* many others


Sources:
Python â€“ Downloading captions from YouTube
https://www.geeksforgeeks.org/python-downloading-captions-from-youtube/
